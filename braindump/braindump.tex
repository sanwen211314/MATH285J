
\documentclass[12pt]{article}
\usepackage[top=1 in, bottom=1 in, left=1 in, right = 1 in]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}

\newcommand{\Corr}{\text{Corr}}



\begin{document}
If we observe enough of the entries (80\% in this example), we get perfect reconstruction for sparse matrices with relatively high probability (perfect reconstruction roughly half the time) using nuclear norm minimization $$\min \| X \|_* \,\,\,\, \text{subject to} \,\,\,\, P_\Omega(X) = P_{\Omega}(M).$$ 

\begin{center}
\includegraphics[width=\textwidth]{sparseReconstruction.png}\\
\end{center}

\noindent\makebox[\linewidth]{\rule{\textwidth}{2pt}}

When we sample much less (30\% in this example), we no longer get perfect reconstruction, but we do preserve some of the structure of the matrix. Note the entries circled in red: these are large entries in the original matrix and, though they are unobserved, the reconstruction matches them fairly well. [Of course, the reconstruction also misses some large unobserved entries.
  
\begin{center}
\includegraphics[width=\textwidth]{sparseReconstructionLowObv.png}\\
\end{center}  
  
  
\noindent\makebox[\linewidth]{\rule{\textwidth}{2pt}}  

Moving to dense matrices, we cannot expect perfect reconstruction even when observing 80\% of entries.
  
\begin{center}
\includegraphics[width=\textwidth]{denseReconstruction.png}\\
\end{center}  
  
\noindent\makebox[\linewidth]{\rule{\textwidth}{2pt}}  

We want to consider matrices with certain columns correlated. As a first attempt, we let the first ten columns be "perfectly correlated" (i.e., they are all constant multiples of a single vector so that their correlation matrix is a $10\times 10$ matrix where every entry is $1$). In this case, we can selectively sample the entries. Here we sample 30\% of the entire matrix (120 entries), but we take only 20\% (24 entries) of these samples from the correlated columns and the remaining 80\% (96 entries) from the uncorrelated columns. Note, we are still using the nuclear norm minimization $$\min \| X \|_* \,\,\,\, \text{subject to} \,\,\,\, P_\Omega(X) = P_{\Omega}(M).$$ This selective sampling does worse than if we just sample evenly in the same scenario. This should be expected since we haven't actually used the correlation yet.
  
 \begin{center}
\includegraphics[width=\textwidth]{selectSample1.png}\\
\end{center}  
  
\noindent\makebox[\linewidth]{\rule{\textwidth}{2pt}} 
  
 We tried to think of  ways to incorporate the correlation into the problem. A first thought might be to change the functional to incorporate the correlation matrix itself, perhaps: $$\min \| X \|_* + \gamma \| \Corr(X_\tau) + \Corr(M_\tau)\|^2_F \,\,\,\, \text{subject to} \,\,\,\, P_\Omega(X) = P_{\Omega}(M)$$ where $\tau$ represents the collection of columns that are known to be correlated. However, the map $X\mapsto \Corr(X)$ is non-linear and so that map $X\mapsto \| \Corr(X_\tau) - \Corr(M_\tau)\|_F^2$ is non-convex. If we wanted to do this, we would need some more sophisticated non-convex optimization routines. Instead, it may be better to use some portion of the samples to produce an estimation $\tilde M_\tau$ to $M_\tau$ and then insert a fidelity term in order to match $X_\tau$ to $\tilde M_\tau$ and combine this with nuclear norm minimization: $$\min \| X \|_* + \gamma \| X_\tau + \tilde M_\tau\|^2_F \,\,\,\, \text{subject to} \,\,\,\, P_\Omega(X) = P_{\Omega}(M).$$ This idea shows some promise. Indeed assuming we can produce $\tilde M_\tau$ with some amount of accuracy, we can sample at a much lower rate from the correlated columns since they are accounted for by the fidelity term. Indeed, we did a small parameter sweep and it suggested that taking $\tilde \gamma =1$ and taking all the samples from the uncorrelated columns is ideal (here we assume we have used 40 samples to construct $\tilde M_\tau$ and then take the remaining $80$ samples from the uncorrelated portion). 
 
  \begin{center}
\includegraphics[width=\textwidth]{selectSample1.png}\\
\end{center}  
  
\noindent\makebox[\linewidth]{\rule{\textwidth}{2pt}} 
  
Next we are focusing on how to accurately reconstruct $M_\tau$ with relatively few samples, given that we know that the columns are correlated (or an easier problems: given that we know exactly how the columns are correlated; i.e., we know $\Corr(M_\tau)$ exactly). 

We've found that singular value threshholding (SVT) works pretty well for large, low rank matrices so we may be able to use SVT for $\tilde M_\tau$ if we assume that $M_\tau$ is low-rank. In the following picture, we are using SVT to approximately reconstruct at rank 1, $300 \times 300$ matrix. We are still not using any information about the correlation (How to incorporate this into SVT?) but even so the reconstruction is very good when we are only observing 10\% of the entries.

  \begin{center}
\includegraphics[width=0.7\textwidth]{SVT.png}\\
\end{center}  

\noindent\makebox[\linewidth]{\rule{\textwidth}{2pt}} 
 
Otherwise, we may be able to use multivariate regression to identify the correlation. In this method, given that we know that which columns are correlated, we assume that they are linearly correlated. Then, we obtain a multivariate linear regression by sampling the minimum amount of rows from the correlated columns such that the covariance matrix is positive-definite. After that, we sample from the independent columns and use these coefficients to obtain best estimates for the corresponding rows of the dependent columns. These amount of samples normally add to about 5\% of observations depending on the particular observation rate, with the other 95\% we use to sample the non-correlated columns. The multivariate linear regression provides us with the hidden coefficients relating the correlated columns, which we then combine with the nuclear norm minimization as additional optimization constraints. In the following objective function, assuming that $n$ columns are correlated with each other, we treat the elements in the first column as the independent variables and the elements in the next $n - 1$ columns as the dependent variables. $$\min \| X \|_* \,\,\,\, \text{subject to} \,\,\,\, P_\Omega(X) = P_{\Omega}(M) \text{ and } X_{k,j} = X_{k,i} * \beta_{(i,j)_0} + \beta_{(i,j)_1} \forall i, j \text{ where columns j are dependent on columns i}$$

  \begin{center}
\includegraphics[width=1\textwidth]{multivariateLinearRegression.jpg}\\
\end{center}  

  \begin{center}
\includegraphics[width=1\textwidth]{nonAdaptiveMultivariateComparison.jpg}\\
\end{center}  

Our preliminary results are shown above for a 200 x 200 matrix, with 10 trials for each method. As we can see, adaptive sampling (the top one) improves the reconstruction error from 1.53e+01 to 1.42e+01 in comparison to the random sampling method. Also, the red rectangle in the adaptive sampling method highlights how most of the correlated columns remains unobserved.

We can always decrease the reconstruction error in the correlated columns by sampling more from the correlated columns to obtain a better estimate for the correlation. Next, we need to loosen the assumption that we know which rows/columns are correlated, and see if we can learn this as we sample. For example, we could sample coarsely and obtain a rough estimation. Then we could examine the correlation pattern from the estimation and adjust the sampling scheme accordingly, then repeat. Even further down the road, we can loosen the assumption that the correlations are linear.

 
 
\end{document}